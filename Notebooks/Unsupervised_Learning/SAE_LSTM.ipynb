{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## A pipeline for training and evaluating a Stacked Autoencoder with LSTM layers for anomaly detection in network traffic data. \n","#It preprocesses the data, trains the model, evaluates its performance, and saves the model for future use."]},{"cell_type":"markdown","metadata":{"id":"8ZEeLGOdrwHl"},"source":["#Data Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMjQMmm5ry5n"},"outputs":[],"source":["# Importing the libraries\n","import numpy as np\n","import pandas as pd\n","import ipaddress\n","\n","#Variables\n","noise_factor = 0.5\n","sample_size = 65000\n","#0 => Normal, 1 => Attack\n","\n","#Load the dataset\n","def loadDataset(datasetFilePath):\n","    train_df =  pd.read_csv(datasetFilePath)\n","    train_df.drop(columns=['Flow ID', 'Src IP', 'Dst IP', 'Timestamp'], inplace = True)\n","    train_df.replace([np.inf, -np.inf], -1, inplace=True)\n","    train_df.replace([np.nan], 0, inplace=True)\n","    ae_train_df = train_df[train_df['Label']=='NormalTraffic']\n","    \n","    return ae_train_df\n","\n","#DDoS Features were found to be 22, 23, 25, 27, 30, 32, 58, 79. So removed them along with the flow ID and the timestamp\n","def getEncoderInput(datasetType, dataset, start, nSamples, nColumns):\n","    if 'unb15' in datasetType or 'custom' in datasetType:    \n","        X = dataset.iloc[start:nSamples, 0:nColumns-2].values    \n","    else:\n","        X = dataset.iloc[start:nSamples, 0:nColumns-1].values   \n"," \n","\n","    if 'cicids2017' in datasetType:\n","        X = np.delete(X, [0, 6, 22, 23, 25, 27, 30, 32, 58, 79], axis=1)\n","        for i in range(len(X)):\n","            X[i, 0] = int(ipaddress.ip_address(X[i, 0]))\n","            X[i, 2] = int(ipaddress.ip_address(X[i, 2]))\n","        #X[i, 5] = int(X[i, 5])\n","    elif 'cicids2018' in datasetType:\n","        X = np.delete(X, [2, 3, 18, 19, 21, 23, 26, 28, 54, 75], axis=1)\n","\n","    # Randomly sampling code - Sailik\n","    #X = dt[np.random.choice(dt.shape[0], sample_size , replace=False), :] \n","    return X\n","\n","\n","def getEncoderLabelCoulmn(datasetType, dataset, start, nSamples):\n","    if 'unb15' in datasetType or 'custom' in datasetType:    \n","        labelIndex = -2    \n","    else:\n","        labelIndex = -1\n","    y = dataset.iloc[start:nSamples, labelIndex].values \n","    integerY = []\n","    for i in range(len(y)):\n","         integerY.append(int(str.lower(str(y[i])) != \"benign\" and str.lower(str(y[i])) != \"normal\" and str.lower(str(y[i])) != \"normaltraffic\"))\n","    y = np.array(integerY)    \n","    y = np.reshape(y, (y.shape[0], 1))\n","    return y\n","\n","#Add noise in case you want to make your model to still be able to contruct the original input (a process known as denoising). Resulting model will fall under Denoising Stacked Autoencoders\n","def addNoise(X):\n","     return X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape)    \n","\n","#If you are using LSTM in your stacked encoder, you have to convert the input into sequences\n","def getEncoderInputSequence(X, nTimesteps, nColumns):\n","    X_sequence = []\n","    for i in range(nTimesteps, np.shape(X)[0]):\n","        X_sequence.append(X[i-nTimesteps:i, :])  \n","    X_sequence = np.array(X_sequence)\n","    X_sequence = np.reshape(X_sequence, (X_sequence.shape[0], X_sequence.shape[1], nColumns))\n","    return X_sequence\n"]},{"cell_type":"markdown","metadata":{"id":"gln8QxHrr1zu"},"source":["# Model Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJXMPzeDr6_u"},"outputs":[],"source":["from keras.layers import Input, LSTM, RepeatVector, Dense\n","#regularizers\n","from keras.models import Model\n","\n","#Get the model \n","def getSAE_LSTM(nTimesteps, nOperatingColumns):\n","    #This is the size of our encoded representations\n","    encoding_dim1 = 60 \n","    encoding_dim2 = 35\n","    encoding_dim3 = 20\n","    \n","    # this is our input placeholder\n","    input = Input(shape=(nTimesteps, nOperatingColumns))\n","    # \"encoded\" is the encoded representation of the input\n","    encoded = LSTM(encoding_dim1, return_sequences=True, dropout = 0.2)(input)\n","    \n","\t#dropout will randomly make some cells void in generating the output. Makes the model better.\n","    encoded = LSTM(encoding_dim2, return_sequences=True, dropout = 0.2)(encoded)\n","    \n","\t#return_sequences passes the sequences to the next layer. Since we have LSTM layers all the way, we need to pass the sequences to the next layers too. \n","    encoded = LSTM(encoding_dim3, return_sequences=True, dropout = 0.2)(encoded)\n","    \n","    decoded = LSTM(encoding_dim2, return_sequences=True, dropout = 0.2)(encoded)\n","    \n","    decoded = LSTM(encoding_dim1, return_sequences=True, dropout = 0.2)(decoded)\n","    \n","    decoded = LSTM(nOperatingColumns, return_sequences=True)(decoded)\n","    \n","    # this model maps an input to its reconstruction\n","    sae_lstm = Model(input, decoded)\n","    \n","    return sae_lstm\n"]},{"cell_type":"markdown","metadata":{"id":"2sHh6W04r93h"},"source":["# Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L7dmQayFsDZh","outputId":"46ad0c38-9dc1-4459-dfdf-8b5033ce8727"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device mapping: no known devices.\n","Epoch 1/10\n","5973/5973 [==============================] - ETA: 0s - loss: 0.0094\n","Epoch 1: loss improved from inf to 0.00944, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 190s 28ms/step - loss: 0.0094 - val_loss: 0.0210\n","Epoch 2/10\n","5971/5973 [============================>.] - ETA: 0s - loss: 0.0048\n","Epoch 2: loss improved from 0.00944 to 0.00478, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 154s 26ms/step - loss: 0.0048 - val_loss: 0.0191\n","Epoch 3/10\n","5972/5973 [============================>.] - ETA: 0s - loss: 0.0040\n","Epoch 3: loss improved from 0.00478 to 0.00398, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 150s 25ms/step - loss: 0.0040 - val_loss: 0.0189\n","Epoch 4/10\n","5973/5973 [==============================] - ETA: 0s - loss: 0.0035\n","Epoch 4: loss improved from 0.00398 to 0.00347, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 147s 25ms/step - loss: 0.0035 - val_loss: 0.0187\n","Epoch 5/10\n","5973/5973 [==============================] - ETA: 0s - loss: 0.0028\n","Epoch 5: loss improved from 0.00347 to 0.00282, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 156s 26ms/step - loss: 0.0028 - val_loss: 0.0187\n","Epoch 6/10\n","5971/5973 [============================>.] - ETA: 0s - loss: 0.0026\n","Epoch 6: loss improved from 0.00282 to 0.00257, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 150s 25ms/step - loss: 0.0026 - val_loss: 0.0185\n","Epoch 7/10\n","5971/5973 [============================>.] - ETA: 0s - loss: 0.0024\n","Epoch 7: loss improved from 0.00257 to 0.00237, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 164s 28ms/step - loss: 0.0024 - val_loss: 0.0183\n","Epoch 8/10\n","5972/5973 [============================>.] - ETA: 0s - loss: 0.0022\n","Epoch 8: loss improved from 0.00237 to 0.00225, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 152s 25ms/step - loss: 0.0022 - val_loss: 0.0184\n","Epoch 9/10\n","5972/5973 [============================>.] - ETA: 0s - loss: 0.0022\n","Epoch 9: loss improved from 0.00225 to 0.00216, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 155s 26ms/step - loss: 0.0022 - val_loss: 0.0183\n","Epoch 10/10\n","5971/5973 [============================>.] - ETA: 0s - loss: 0.0021\n","Epoch 10: loss improved from 0.00216 to 0.00209, saving model to sae_lstm.h5\n","5973/5973 [==============================] - 155s 26ms/step - loss: 0.0021 - val_loss: 0.0182\n","Saved model to disk\n"]}],"source":["import math\n","from keras.models import Model, model_from_json\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from numpy.testing import assert_allclose\n","from keras.models import load_model\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.utils.vis_utils import plot_model\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import os\n","\n","# Specify which GPU(s) to use\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Or 2, 3, etc. other than 0\n","\n","# On CPU/GPU placement\n","config = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n","config.gpu_options.allow_growth = True\n","tf.compat.v1.Session(config=config)\n","\n","\n","dataset_train = loadDataset(\"SCVIC_APT/Training.csv\")\n","datasetType = 'other'\n","_nTotal = dataset_train.shape[0]\n","_nColumns = dataset_train.shape[1]\n","#Using 75% of the data for training and remaining 25% for validation testing\n","_nSamplesTrain = math.ceil(_nTotal * 0.75)\n","_nSamplesValidation = _nTotal - _nSamplesTrain\n","_nTimesteps = 3\n","\n","X_train  = getEncoderInput(datasetType, dataset_train, 0, _nSamplesTrain, _nColumns)\n","#X_train = getEncoderInput(dataset_train, 0, _nSamplesTrain, _nColumns)\n","#y = getEncoderLabelCoulmn(dataset_train, 0, _nSamplesTrain, _nColumns)\n","y = getEncoderLabelCoulmn(datasetType, dataset_train, 0, _nSamplesTrain)\n","\n","Validation_X = getEncoderInput(datasetType, dataset_train, _nSamplesTrain, _nSamplesTrain+_nSamplesValidation, _nColumns)\n","\n","# Feature Scaling -Normalization recommended for RNN    \n","sc = MinMaxScaler(feature_range = (0, 1))\n","X_train = sc.fit_transform(X_train)\n","Validation_X = sc.fit_transform(Validation_X)\n","\n","#Converting training inputs into LSTM training inputs\n","_nOperatingColumns = len(X_train[0])\n","X_train_sequence = getEncoderInputSequence(X_train, _nTimesteps, _nOperatingColumns)\n","Validation_X_sequence = getEncoderInputSequence(Validation_X, _nTimesteps, _nOperatingColumns)\n","\n","sequence_autoencoder_semi = getSAE_LSTM(_nTimesteps, _nOperatingColumns)\n","sequence_autoencoder_semi.compile(optimizer='adam', loss='mean_squared_error')\n","\n","modelName = \"sae_lstm\"\n","\n","#Adding checkpoints\n","checkpointFile = modelName + \".h5\"\n","checkpoint = ModelCheckpoint(checkpointFile, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","#earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n","#callbacks_list = [checkpoint, earlyStopping]\n","callbacks_list = [checkpoint]\n","\n","#sequence_autoencoder_semi = load_model(checkpointFile)\n","#sequence_autoencoder_semi.summary()\n","plot_model(sequence_autoencoder_semi, to_file='sae_lstm_model_plot.png', show_shapes=True, show_layer_names=True)\n","#Training autoencoder\n","sequence_autoencoder_semi_history = sequence_autoencoder_semi.fit(X_train_sequence, X_train_sequence,\n","                     epochs=10,\n","                     batch_size=32,\n","                     shuffle=False,\n","                     validation_data=(Validation_X_sequence, \n","                     Validation_X_sequence), \n","                     callbacks=callbacks_list)\n","\n","\n","\n","# Save the model and serialize model to JSON and h5\n","sequence_autoencoder_semi.save( modelName + \".h5\")\n","print(\"Saved model to disk\")\n","\n","# loss = sequence_autoencoder_semi_history.history['loss']\n","# val_loss = sequence_autoencoder_semi_history.history['val_loss']\n","# epochs = range(3)\n","# plt.figure()\n","# plt.plot(epochs, loss, color='red', label='Training loss')\n","# plt.plot(epochs, val_loss, color='blue', label='Validation loss')\n","# plt.title('Training and Validation loss')\n","# plt.xlabel('epochs')\n","# plt.ylabel('loss')\n","# plt.legend()\n","# #plt.savefig('LossColored_' + modelName + '.png')\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"TWhkS9tBFMIp"},"source":["# Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoxTYPfGGDD_","outputId":"ad3e36f1-11c9-4b60-f2a8-0fd086a596e2"},"outputs":[{"data":{"text/plain":["['preprocess_pipeline_SAE_LSTM.save']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import joblib\n","preprocess_filename = \"preprocess_pipeline_SAE_LSTM.save\"\n","joblib.dump(sc, preprocess_filename) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuOa1NXDG-AI"},"outputs":[],"source":["import torch\n","from sklearn.metrics import classification_report, roc_auc_score, average_precision_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ITl_MvHFelH","outputId":"d6cc8c3a-d9d5-4569-b1a5-483c04846618"},"outputs":[{"name":"stdout","output_type":"stream","text":["1766/1766 [==============================] - 15s 7ms/step\n","0.8266275471535888\n","0.04491375707595505\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.98      0.98     55580\n","           1       0.05      0.05      0.05       904\n","\n","    accuracy                           0.97     56484\n","   macro avg       0.52      0.52      0.52     56484\n","weighted avg       0.97      0.97      0.97     56484\n","\n","1749/1749 [==============================] - 13s 7ms/step\n","0.8246257646635481\n","0.018174234875925614\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99     55580\n","           1       0.01      0.01      0.01       360\n","\n","    accuracy                           0.99     55940\n","   macro avg       0.50      0.50      0.50     55940\n","weighted avg       0.99      0.99      0.99     55940\n","\n","1745/1745 [==============================] - 13s 7ms/step\n","0.8247972485731777\n","0.01275628599114658\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     55580\n","           1       0.00      0.00      0.00       251\n","\n","    accuracy                           0.99     55831\n","   macro avg       0.50      0.50      0.50     55831\n","weighted avg       0.99      0.99      0.99     55831\n","\n","1742/1742 [==============================] - 13s 7ms/step\n","0.827832676157702\n","0.00738335172183557\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     55580\n","           1       0.00      0.00      0.00       142\n","\n","    accuracy                           0.99     55722\n","   macro avg       0.50      0.50      0.50     55722\n","weighted avg       0.99      0.99      0.99     55722\n","\n","1740/1740 [==============================] - 13s 7ms/step\n","0.8290482187837351\n","0.00388880130327395\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     55580\n","           1       0.00      0.00      0.00        74\n","\n","    accuracy                           1.00     55654\n","   macro avg       0.50      0.50      0.50     55654\n","weighted avg       1.00      1.00      1.00     55654\n","\n","1740/1740 [==============================] - 16s 9ms/step\n","0.8290482187837351\n","0.004045817570407734\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     55580\n","           1       0.00      0.00      0.00        77\n","\n","    accuracy                           1.00     55657\n","   macro avg       0.50      0.50      0.50     55657\n","weighted avg       1.00      1.00      1.00     55657\n","\n"]}],"source":["test_df =  pd.read_csv(\"SCVIC_APT/Testing.csv\")\n","test_df.drop(columns=['Flow ID', 'Src IP', 'Dst IP', 'Timestamp'], inplace = True)\n","test_df.replace([np.inf, -np.inf], -1, inplace=True)\n","test_df.replace([np.nan], 0, inplace=True)\n","\n","pivoting_test_df = test_df[test_df['Label'].isin(['NormalTraffic', 'Pivoting'])]\n","\n","reconnaissance_test_df = test_df[test_df['Label'].isin(['NormalTraffic', 'Reconnaissance'])]\n","\n","lateralmovement_test_df = test_df[test_df['Label'].isin(['NormalTraffic', 'LateralMovement'])]\n","\n","dataexfiltration_test_df = test_df[test_df['Label'].isin(['NormalTraffic', 'DataExfiltration'])]\n","\n","initialcompromise_test_df = test_df[test_df['Label'].isin(['NormalTraffic', 'InitialCompromise'])]\n","\n","\n","datasetType = 'other'\n","_nTotal = test_df.shape[0]\n","_nColumns = test_df.shape[1]\n","\n","_nTimesteps = 3\n","\n","for dataset_test in [test_df, pivoting_test_df, reconnaissance_test_df, lateralmovement_test_df, dataexfiltration_test_df, initialcompromise_test_df]:\n","    X_test  = getEncoderInput(datasetType, dataset_test, 0, _nTotal, _nColumns)\n","    #X_train = getEncoderInput(dataset_train, 0, _nSamplesTrain, _nColumns)\n","    #y = getEncoderLabelCoulmn(dataset_train, 0, _nSamplesTrain, _nColumns)\n","    y_test = getEncoderLabelCoulmn(datasetType, dataset_test, 0, _nTotal)[:-_nTimesteps]\n","\n","    # Feature Scaling -Normalization recommended for RNN    \n","    X_test = sc.transform(X_test)\n","\n","    #Converting training inputs into LSTM training inputs\n","    _nOperatingColumns = len(X_test[0])\n","    X_test_sequence = getEncoderInputSequence(X_test, _nTimesteps, _nOperatingColumns)\n","\n","    # pass the transformed test set through the autoencoder to get the reconstructed result\n","    reconstructions = sequence_autoencoder_semi.predict(X_test_sequence)\n","\n","\n","    dim = tf.reduce_prod(tf.shape(X_test_sequence)[1:])\n","    X_test_sequence_flatten = tf.reshape(X_test_sequence, [-1, dim])\n","\n","    dim = tf.reduce_prod(tf.shape(reconstructions)[1:])\n","    reconstructions_flatten = tf.reshape(reconstructions, [-1, dim])\n","\n","    gap_loss = torch.mean(torch.nn.functional.mse_loss(torch.FloatTensor(X_test_sequence_flatten.numpy()), torch.FloatTensor(reconstructions_flatten.numpy()), reduction='none'), dim=1)\n","\n","    print(roc_auc_score(y_test, gap_loss.detach().numpy()))\n","    print(average_precision_score(y_test, gap_loss.detach().numpy()))\n","\n","    ind = np.argpartition(gap_loss, -sum(y_test))[-sum(y_test)[0]:]\n","\n","    top_k = np.zeros(gap_loss.shape)\n","\n","    top_k[ind] = 1\n","\n","    print(classification_report(y_test,top_k))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w62hFAmbQo9n"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
